{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayDXtC0HhCNA",
        "outputId": "22595780-9824-458f-a1ff-97ab43440ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.21.1 rapidfuzz-3.1.2\n"
          ]
        }
      ],
      "source": [
        "pip install Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y_m--loxPAll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b271ba20-6ba8-4be0-d5e9-1a4e9943ba16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "#from thefuzz import fuzz\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4MJDrwsbPH65"
      },
      "outputs": [],
      "source": [
        "corpus = open('corpus.txt').read()\n",
        "corpus=re.sub(\"[^a-zA-Z-' ]\", \"\", corpus)\n",
        "tokenized_words = word_tokenize(corpus)\n",
        "\n",
        "fdist = nltk.FreqDist(w.lower() for w in tokenized_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenized_words = set(tokenized_words)"
      ],
      "metadata": {
        "id": "FkYjF1dVeWEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_candidates(token, max_edit_distance=1):\n",
        "\n",
        "    candidates =[(word,levenshtein_distance(token, word)) for word in fdist if levenshtein_distance(token, word) <= max_edit_distance]\n",
        "    #candidates = [(word,fuzz.ratio(token, word)) for word in tokenized_words if min_sim < fuzz.ratio(token, word)]\n",
        "\n",
        "    candidates = sorted(candidates, key=lambda x: fdist[x], reverse=True) # sort base on the number of repetition in tokenized_word\n",
        "\n",
        "    #candidates = sorted(candidates,key=lambda tup: tup[1], reverse=True)\n",
        "    return candidates\n"
      ],
      "metadata": {
        "id": "vvUnFhtNcafL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rku7xs9cYFBj"
      },
      "outputs": [],
      "source": [
        "def correct_text(text,ground_truth):\n",
        "\n",
        "    text=re.sub(\"[^a-zA-Z-' ]\", \"\", text)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    corrected_tokens = []\n",
        "    new_tokens = []\n",
        "    corrected_text = []\n",
        "    detected_errors = 0\n",
        "    corrected_errors = 0\n",
        "\n",
        "    for token in tokens:\n",
        "        candidates = generate_candidates(token)\n",
        "        print(candidates)\n",
        "        if not candidates: # the tokon not in dictionary\n",
        "            new_tokens.append(token.lower())\n",
        "            corrected_text.append(token.lower())\n",
        "            continue\n",
        "\n",
        "        else:\n",
        "          # best_score = candidates[0][1] # the token is right\n",
        "          # if best_score == 0:\n",
        "          #   corrected_text.append(token.lower())\n",
        "          #   continue\n",
        "\n",
        "          best_suggestion = candidates[0][0]# correct the token\n",
        "          corrected_tokens.append(best_suggestion.lower())\n",
        "          corrected_text.append(best_suggestion.lower())\n",
        "\n",
        "          #corrected_errors += 1\n",
        "\n",
        "\n",
        "          detected_errors += 1\n",
        "\n",
        "    total_tokens = len(tokens)\n",
        "    corrected_errors = (sum(1 for i in range(total_tokens) if corrected_text[i].lower() != tokens[i].lower()))\n",
        "    print(\"Detected Errors:\", detected_errors)\n",
        "    print(\"Corrected Errors:\", corrected_errors)\n",
        "    print(\"New Tokens:\", len(new_tokens))\n",
        "\n",
        "\n",
        "    correct_tokens = sum(1 for i in range(total_tokens) if corrected_text[i].lower() == ground_truth[i].lower())\n",
        "    accuracy = (correct_tokens / total_tokens) * 100\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # #Calculate accuracy\n",
        "    # total_tokens = len(tokens)\n",
        "    # correct_tokens = total_tokens - (len(new_tokens)\n",
        "    # accuracy = (correct_tokens / total_tokens) * 100\n",
        "    # print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    return \" \".join(corrected_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insertion**:The user inserts one extra character somewhere in the string\n"
      ],
      "metadata": {
        "id": "ouseUV3LTCk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = input(\"Please enter the text: \")\n",
        "ground_truth = input(\"Please enter the ground truth words (space-separated): \").split()\n",
        "\n",
        "corrected_text = correct_text(text, ground_truth)\n",
        "print(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDDDAVrInJnc",
        "outputId": "d39b3efa-4851-4f60-dd85-d66902f216bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the text: Affter reeiving heer visitor\n",
            "Please enter the ground truth words (space-separated): After receiving her visitor\n",
            "[('iffter', 1)]\n",
            "[('receiving', 1), ('reeiving', 0), ('reviving', 1)]\n",
            "[('her', 1), ('hear', 1), ('heel', 1), ('heed', 1), ('heir', 1), ('sheer', 1), ('beer', 1), ('heek', 1), ('heder', 1), ('cheer', 1), ('eer', 1), ('deer', 1), ('herr', 1), ('peer', 1), ('theer', 1), ('heer', 0), ('heers', 1), ('heker', 1)]\n",
            "[('visitor', 0), ('visitors', 1), ('avisitor', 1)]\n",
            "Detected Errors: 4\n",
            "Corrected Errors: 3\n",
            "New Tokens: 0\n",
            "Accuracy: 75.0\n",
            "iffter receiving her visitor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kJV-xmtGMJPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deletion**: One character in the string is deleted incorrectly"
      ],
      "metadata": {
        "id": "2rqpBtC_STP4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ov9OgnOmM78R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Please enter the text: \")\n",
        "ground_truth = input(\"Please enter the ground truth words (space-separated): \").split()\n",
        "\n",
        "corrected_text = correct_text(text, ground_truth)\n",
        "print(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axXEYfp1TRLE",
        "outputId": "a2ddb515-243b-4858-fd75-cecf31a45117"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the text: you must returned fro by benefactor\n",
            "Please enter the ground truth words (space-separated): you must returned from by benefactor\n",
            "[('you', 0), ('your', 1), (\"'you\", 1), ('youi', 1), ('youo', 1), ('lou', 1), ('iyou', 1), ('ou', 1), ('yout', 1), ('iou', 1), ('yon', 1)]\n",
            "[('must', 0), ('most', 1), ('just', 1), ('mist', 1), ('dust', 1), ('muste', 1), ('lust', 1), ('musi', 1), ('imust', 1), ('bust', 1), ('rust', 1), ('ust', 1), ('muse', 1), ('fust', 1), ('gust', 1), ('murt', 1), ('muss', 1), ('mast', 1)]\n",
            "[('returned', 0), ('returne', 1), ('returnnd', 1), ('returnedi', 1), ('retuned', 1)]\n",
            "[('from', 1), ('fr', 1), ('fro', 0), ('foo', 1), ('frm', 1), ('frog', 1), ('pro', 1), ('fre', 1), ('ro', 1), ('faro', 1), ('fwo', 1), ('fo', 1), ('tro', 1)]\n",
            "[('by', 0), ('be', 1), ('my', 1), ('y', 1), ('ay', 1), ('boy', 1), ('ny', 1), ('sy', 1), ('wy', 1), ('buy', 1), ('b', 1), ('ly', 1), ('ry', 1), ('bay', 1), ('oy', 1), ('py', 1), ('byn', 1), ('hy', 1), ('aby', 1), ('jy', 1), (\"'by\", 1), ('gy', 1), ('uy', 1), ('byi', 1), ('bt', 1), ('vy', 1), ('bya', 1), ('xy', 1), ('bl', 1), ('bc', 1), ('br', 1), ('ty', 1)]\n",
            "[('benefactor', 0), ('benefactors', 1), ('enefactor', 1)]\n",
            "Detected Errors: 6\n",
            "Corrected Errors: 1\n",
            "New Tokens: 0\n",
            "Accuracy: 100.0\n",
            "you must returned from by benefactor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Replacement**:One character in the string is incorrectly replaced by another one"
      ],
      "metadata": {
        "id": "aBMo5tHfWJd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Please enter the text: \")\n",
        "ground_truth = input(\"Please enter the ground truth words (space-separated): \").split()\n",
        "\n",
        "corrected_text = correct_text(text, ground_truth)\n",
        "print(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2fcOsQmWREu",
        "outputId": "471aee4a-5020-4379-f56c-0d09d9cb8d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the text: snly to this aim cam we always ftrive independentli\n",
            "Please enter the ground truth words (space-separated): Only to this aim can we always strive independently\n",
            "[('only', 1), ('sly', 1), ('mnly', 1), ('sdly', 1), ('slly', 1)]\n",
            "[('to', 0), ('so', 1), ('no', 1), ('do', 1), ('two', 1), ('go', 1), ('too', 1), ('t', 1), ('o', 1), ('wo', 1), ('toe', 1), ('th', 1), ('tol', 1), ('top', 1), ('te', 1), ('io', 1), ('ton', 1), ('ho', 1), ('toy', 1), ('tom', 1), (\"'to\", 1), ('tx', 1), ('toi', 1), ('co', 1), ('oo', 1), ('ito', 1), ('ti', 1), ('vo', 1), ('tos', 1), ('jo', 1), ('mo', 1), ('tp', 1), ('ro', 1), ('tr', 1), ('tm', 1), ('oto', 1), ('tu', 1), ('fo', 1), (\"'o\", 1), ('tow', 1), ('tt', 1), ('tro', 1), ('lo', 1), ('ta', 1), ('ty', 1), ('po', 1)]\n",
            "[('his', 1), ('this', 0), ('thus', 1), ('thin', 1), ('thir', 1), ('tohis', 1), ('athis', 1), ('thik', 1), (\"'this\", 1), (\"'his\", 1), ('tis', 1), ('zthis', 1), ('thins', 1), ('thisy', 1), ('ithis', 1), ('this-', 1), ('xthis', 1), ('thisi', 1), ('ethis', 1)]\n",
            "[('him', 1), ('am', 1), ('arm', 1), ('air', 1), ('aid', 1), ('aim', 0), ('im', 1), ('lim', 1), ('dim', 1), ('aims', 1), ('alm', 1), ('rim', 1), ('laim', 1), ('ai', 1), ('aime', 1), ('jim', 1), ('alim', 1), ('aix', 1), ('maim', 1), ('ait', 1), ('ail', 1), ('tim', 1)]\n",
            "[('can', 1), ('came', 1), ('am', 1), ('ca', 1), ('cap', 1), ('calm', 1), ('camp', 1), ('cab', 1), ('dam', 1), ('cat', 1), ('ram', 1), ('ham', 1), ('sam', 1), ('car', 1), ('cm', 1), ('jam', 1), ('cal', 1), (\"'am\", 1), ('eam', 1), ('cum', 1)]\n",
            "[('he', 1), ('be', 1), ('me', 1), ('we', 0), ('ws', 1), ('re', 1), ('e', 1), ('wo', 1), ('de', 1), ('wr', 1), ('wy', 1), ('wet', 1), ('le', 1), ('ie', 1), ('owe', 1), ('web', 1), ('te', 1), (\"'we\", 1), ('se', 1), ('ue', 1), ('ge', 1), ('w', 1), ('je', 1), ('wie', 1), ('ne', 1), ('ee', 1), ('ze', 1), ('ce', 1), ('awe', 1), ('fe', 1), ('ye', 1), ('wek', 1), ('weo', 1), ('wee', 1), ('wm', 1), ('wh', 1), ('wer', 1), ('wve', 1), ('ve', 1), ('wke', 1), ('wge', 1), ('oe', 1), ('-e', 1), ('wf', 1), ('wt', 1), ('wb', 1), ('wj', 1), ('wg', 1), ('wc', 1), ('wd', 1), ('wen', 1), ('pe', 1), ('wa', 1), ('wre', 1), ('woe', 1), ('wed', 1)]\n",
            "[('always', 0), ('lways', 1), ('allays', 1), ('alwayso', 1), ('amlways', 1)]\n",
            "[('strive', 1)]\n",
            "[('independently', 1)]\n",
            "Detected Errors: 9\n",
            "Corrected Errors: 7\n",
            "New Tokens: 0\n",
            "Accuracy: 66.66666666666666\n",
            "only to his him can he always strive independently\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transposition**: The user swaps a pair of consecutive characters. Example: The user enters Noramlly instead of Normally."
      ],
      "metadata": {
        "id": "-bPhCrkwVHe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = input(\"Please enter the text: \")\n",
        "ground_truth = input(\"Please enter the ground truth words (space-separated): \").split()\n",
        "\n",
        "corrected_text = correct_text(text, ground_truth)\n",
        "print(corrected_text)\n"
      ],
      "metadata": {
        "id": "JpppmSGNYiCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e32bc817-e00d-4662-9e5a-c056c5e36eef"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the text:  have seen taht was represent\n",
            "Please enter the ground truth words (space-separated):  have seen that was represent\n",
            "[('have', 0), ('hve', 1), ('gave', 1), ('save', 1), ('hare', 1), ('hive', 1), ('wave', 1), ('hate', 1), ('haven', 1), ('haze', 1), ('shave', 1), ('ehave', 1), ('hase', 1), ('heave', 1), ('cave', 1), ('rave', 1), (\"'have\", 1), ('ha-ve', 1), ('ave', 1), ('dave', 1)]\n",
            "[('been', 1), ('see', 1), ('seen', 0), ('seven', 1), ('seem', 1), ('seek', 1), ('een', 1), ('keen', 1), ('sees', 1), ('sen', 1), ('seed', 1), ('sewn', 1), ('sreen', 1), ('spen', 1), ('semen', 1), ('sean', 1), ('sheen', 1), ('seei', 1), (\"s'en\", 1), ('sein', 1), ('seent', 1), ('seecn', 1), ('geen', 1)]\n",
            "[('tht', 1), ('taft', 1), ('tact', 1), ('tat', 1)]\n",
            "[('was', 0), ('as', 1), ('has', 1), ('ws', 1), ('way', 1), ('war', 1), ('wars', 1), ('ways', 1), ('wax', 1), ('pas', 1), ('wash', 1), ('gas', 1), ('wrs', 1), ('wasn', 1), ('wass', 1), ('wys', 1), ('wast', 1), ('wag', 1), (\"'as\", 1), ('ras', 1), ('wat', 1), ('wags', 1), ('wa', 1), ('das', 1), ('fas', 1), ('ias', 1), ('wan', 1), ('wad', 1), ('las', 1)]\n",
            "[('represent', 0), ('represents', 1), ('arepresent', 1), ('bepresent', 1), ('represente', 1)]\n",
            "Detected Errors: 5\n",
            "Corrected Errors: 2\n",
            "New Tokens: 0\n",
            "Accuracy: 60.0\n",
            "have been tht was represent\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}